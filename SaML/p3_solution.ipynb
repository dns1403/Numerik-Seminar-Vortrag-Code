{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenblatt perceptron and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = True  # Use per default a grid, i.e. plt.grid()\n",
    "# mpl.rcParams['figure.figsize'] = [6.4, 4.8]  # Change the default figure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.set_printoptions:\n",
    "#     threshold: Total number of array elements which trigger summarization rather than full repr (default 1000).\n",
    "np.set_printoptions(threshold=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_0_1_to_minus_1_1(labels):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    return 2 * labels - 1\n",
    "# REPLACE\n",
    "map_0_1_to_minus_1_1(np.array([0, 0, 1, 1, 0]))# REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Dataset:\n",
    "    name: str\n",
    "    features: np.array\n",
    "    labels: np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circles_dataset(number_of_samples=1000, random_state=np.random):\n",
    "    labels = random_state.choice([0, 1], size=number_of_samples)\n",
    "    features = random_state.normal(size=(number_of_samples, 2))\n",
    "    features /= np.linalg.norm(features, axis=-1, keepdims=True)\n",
    "    features *= labels[:, None] + 1 / 2 * random_state.uniform(size=number_of_samples)[:, None]\n",
    "    return Dataset('circles', features, map_0_1_to_minus_1_1(labels))\n",
    "\n",
    "def generate_blobs_dataset(number_of_samples=1000, scale=1, random_state=np.random, name='blobs'):\n",
    "    labels = random_state.choice([0, 1], size=number_of_samples)\n",
    "    cluster_centers = random_state.normal(scale=scale, size=(2, 2))\n",
    "    features = cluster_centers[labels, :] + random_state.normal(size=(number_of_samples, 2))\n",
    "    return Dataset(name, features, map_0_1_to_minus_1_1(labels))\n",
    "\n",
    "def generate_checkerboard_dataset(number_of_samples=1000, random_state=np.random):\n",
    "    features = random_state.uniform(-1, 1, size=(number_of_samples, 2))\n",
    "    labels = np.sign(np.prod(features, axis=1)).astype(int)\n",
    "    return Dataset('checkerboard', features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles = generate_circles_dataset(random_state=np.random.RandomState(0))\n",
    "easy_blobs = generate_blobs_dataset(scale=4, random_state=np.random.RandomState(0), name='easy_blobs')\n",
    "checkerboard = generate_checkerboard_dataset(random_state=np.random.RandomState(0))\n",
    "hard_blobs = generate_blobs_dataset(scale=1, random_state=np.random.RandomState(0), name='hard_blobs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the datasets are linearly separable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(dataset, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    classes = np.unique(dataset.labels)\n",
    "    classes = np.sort(classes)\n",
    "    for c in classes:\n",
    "        ax.scatter(\n",
    "            dataset.features[dataset.labels == c, 0],\n",
    "            dataset.features[dataset.labels == c, 1],\n",
    "            label=f'Class: {c}',\n",
    "        )\n",
    "    ax.set_aspect('equal')\n",
    "    if title is None:\n",
    "        title = dataset.name\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "figure, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "plot_dataset(circles, ax=axes[0, 0])\n",
    "plot_dataset(easy_blobs, ax=axes[0, 1])\n",
    "plot_dataset(checkerboard, ax=axes[1, 0])\n",
    "plot_dataset(hard_blobs, ax=axes[1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rosenblatt Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a shorter notation, lets use the following abbreviations ($\\tilde{\\mathbf{x}}_n$ is an \"augmented vector\"):\n",
    "\\begin{align}\n",
    "\\tilde{\\mathbf{w}} &= \\begin{pmatrix}\\mathbf w \\\\ w_0\\end{pmatrix}, &\n",
    "\\tilde{\\mathbf{x}}_n &= \\begin{pmatrix}\\mathbf x_n \\\\ 1\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Apply the Rosenblatt criterion to each of the datasets:\n",
    "\\begin{align}\n",
    "J(\\tilde{\\mathbf{w}}) &= -\\frac 1 N \\sum_{n=1}^N \\left(\\frac{c_n - \\hat c_n}{2}\\right) \\tilde{\\mathbf{w}}^{\\mathsf T} \\tilde{\\mathbf{x}}_n\n",
    "\\end{align}\n",
    "\n",
    "- Make sure to use some stopping criterion to avoid infinite loops.\n",
    "- Plot the training loss over iterations.\n",
    "- For which of the datasets does the training converge?\n",
    "\n",
    "Before you start, think about the following:\n",
    "What are the varaiable `x`, `x_tilde` and `w_tilde`\n",
    "in the code and what are the shapes of them? i.e. think about the relation to the math variables $\\mathbf{x}_n$, $\\tilde{\\mathbf{x}}_n$ and $\\tilde{\\mathbf{w}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_augmentation(x):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \n",
    "    Augmented vector: https://en.wikipedia.org/wiki/Affine_transformation#Augmented_matrix\n",
    "    \"\"\"\n",
    "    N = x.shape[0]  # REPLACE\n",
    "    return np.concatenate([x, np.ones((N, 1))], axis=-1)  # REPLACE return ???\n",
    "\n",
    "def remove_augmentation(x_tilde):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    return x_tilde[:, :-1]  # REPLACE return ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rosenblatt_perceptron(x_tilde, w_tilde, transform_fn=None):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    if transform_fn is not None:\n",
    "        x = remove_augmentation(x_tilde)\n",
    "        x = transform_fn(x)\n",
    "        x_tilde = add_augmentation(x)\n",
    "    \n",
    "    discriminant = np.einsum('d,nd->n', w_tilde, x_tilde)  # REPLACE discriminant = ???\n",
    "    prediction = map_0_1_to_minus_1_1(discriminant > 0)  # REPLACE prediction = ???\n",
    "    # prediction = np.sign(discriminant)  # alternative solution (error at 0 doesn't hurt)  # REPLACE\n",
    "    return discriminant, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rosenblatt_perceptron(dataset, iterations=1000, learning_rate=0.1, transform_fn=None):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    loss_history = np.full(iterations, np.nan)\n",
    "    N = dataset.features.shape[0]\n",
    "    \n",
    "    x = dataset.features\n",
    "    \n",
    "    if transform_fn is not None:\n",
    "        x = transform_fn(dataset.features)\n",
    "        \n",
    "    x_tilde = add_augmentation(x)\n",
    "    \n",
    "    w_tilde = np.random.normal(size=x_tilde.shape[-1])\n",
    "    c = dataset.labels\n",
    "    for iteration in range(iterations):\n",
    "        discriminant, c_hat = predict_rosenblatt_perceptron(x_tilde, w_tilde)  # transform_fn not nessesary, since is transformed\n",
    "        loss_history[iteration] = -1 / N * np.einsum('n,d,nd->', (c - c_hat) / 2, w_tilde, x_tilde)  # REPLACE loss_history[iteration] = ???\n",
    "        # loss_history[iteration] = - np.mean((c - c_hat) / 2 * discriminant)  # alternative solution  # REPLACE\n",
    "        w_tilde = w_tilde + learning_rate * 1 / N * np.einsum('n,nd->d', (c - c_hat) / 2, x_tilde)  # REPLACE w_tilde = ???\n",
    "        # w_tilde = w_tilde + learning_rate * np.mean((c - c_hat)[..., None] / 2 * x_tilde  # alternative solution  # REPLACE\n",
    "    return w_tilde, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_complete_grid(xlim, ylim, steps):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    x, y = np.meshgrid(\n",
    "        np.linspace(*xlim, steps),\n",
    "        np.linspace(*ylim, steps),\n",
    "    )\n",
    "    return x, y, np.stack([x, y], axis=-1)\n",
    "\n",
    "def fit_and_plot(dataset, transform_fn=None, fit_function=fit_rosenblatt_perceptron, \n",
    "                 levels=[0],  # used later for SVM\n",
    "                 **kwargs):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    w_tilde, loss_history = fit_function(\n",
    "        dataset,\n",
    "        transform_fn=transform_fn,\n",
    "        **kwargs,\n",
    "    )\n",
    "    \n",
    "    _, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Plot loss history\n",
    "    axes[0].plot(loss_history)\n",
    "    axes[0].set_title('{}, final loss: {}'.format(dataset.name, loss_history[-1]))\n",
    "    # Use log scale above 1e-5 and linear scale below\n",
    "    axes[0].set_yscale(\"symlog\", linthresh=1e-5)\n",
    "    \n",
    "    \n",
    "    # Plot decision boundary if in area\n",
    "    steps = 100\n",
    "    x, y, features_grid = compute_complete_grid(\n",
    "        (np.min(dataset.features[:, 0]), np.max(dataset.features[:, 0])),\n",
    "        (np.min(dataset.features[:, 1]), np.max(dataset.features[:, 1])),\n",
    "        steps=steps\n",
    "    )\n",
    "    features_grid = features_grid.reshape(steps * steps, 2)\n",
    "    features_grid = add_augmentation(features_grid)\n",
    "    z, _ = predict_rosenblatt_perceptron(features_grid, w_tilde, transform_fn=transform_fn)\n",
    "    z = np.reshape(z, (steps, steps))\n",
    "    \n",
    "    if np.any(z < 0) and np.any(z > 0):\n",
    "        axes[1].contourf(x, y, z >= 0, colors=['cyan', 'yellow'], alpha=0.2)\n",
    "        plot_dataset(dataset, ax=axes[1])\n",
    "        axes[1].contour(x, y, z, levels=levels)\n",
    "    else:\n",
    "        axes[1].contourf(x, y, z >= 0, colors=['cyan', 'yellow'],  alpha=0.2)\n",
    "        plot_dataset(dataset, ax=axes[1])\n",
    "        axes[1].contour(x, y, z)\n",
    "    plt.show()\n",
    "    \n",
    "    print('w_tilde', w_tilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which datasets are linearly separable?\n",
    "- Explain the loss curve for the non linearly separable datasets.\n",
    "- Is the loss curve an indicator if the we can get a reasonable discriminant for non linearly separable datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(circles)\n",
    "fit_and_plot(easy_blobs)\n",
    "fit_and_plot(checkerboard)\n",
    "fit_and_plot(hard_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature transformation\n",
    "\n",
    "Which hand-crafted feature transformation is necessary to render the `circles` and `checkerboard` datasets linearly separable? Train a Rosenblatt perceptron using this transformation. Plot the trainig loss over iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analyse the decision boundarys.\n",
    "- Which solution generalizes better?\n",
    "- For which dataset do you expect a better solution from SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformed_features_dataset(dataset, transform_fn, ax=None):\n",
    "    transformed_features = add_augmentation(transform_fn(dataset.features))\n",
    "    # Add the original features to see the plots for old vs new feature. \n",
    "    transformed_features = np.concatenate([dataset.features, transformed_features], axis=-1)\n",
    "    \n",
    "    if transformed_features.shape[-1] in [2, 3]:\n",
    "        _, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        transformed = Dataset(transformed_features, dataset.labels)\n",
    "        plot_dataset(transformed, ax)\n",
    "    elif 8 > transformed_features.shape[-1] > 2:\n",
    "        # This \"elif\" does the same as the \"if\", except\n",
    "        # that it will plot each parameter combination.\n",
    "        combinations = list(itertools.combinations(range(transformed_features.shape[-1]-1), 2))        \n",
    "        nrows, ncols = (len(combinations)+2) // 3, 3\n",
    "        _, axes = plt.subplots(nrows, ncols, figsize=(6*ncols, 6*nrows), squeeze=False)\n",
    "        for (i, j), ax in zip(combinations, axes.ravel()):\n",
    "            transformed = Dataset(dataset.name, transformed_features[..., [i, j]], dataset.labels)\n",
    "            plot_dataset(transformed, ax)\n",
    "            ax.set_xlabel(f'Original feature {i}' if i < 2 else f'New feature {i-2}')\n",
    "            ax.set_ylabel(f'Original feature {j}' if j < 2 else f'New feature {j-2}')\n",
    "    else:\n",
    "        raise RuntimeError(transformed_features.shape)\n",
    "    \n",
    "plot_transformed_features_dataset(\n",
    "    circles, \n",
    "    transform_fn=lambda x: np.linalg.norm(x, axis=-1, keepdims=True),  # REPLACE transform_fn=lambda x: ???\n",
    ")\n",
    "plot_transformed_features_dataset(\n",
    "    checkerboard, \n",
    "    transform_fn=lambda x: np.prod(x, axis=-1, keepdims=True),  # REPLACE transform_fn=lambda x: ???\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(\n",
    "    circles,\n",
    "    transform_fn=lambda x: np.linalg.norm(x, axis=-1, keepdims=True),  # REPLACE transform_fn=lambda x: ???\n",
    ")\n",
    "fit_and_plot(\n",
    "    checkerboard,\n",
    "    iterations=10000,\n",
    "    transform_fn=lambda x: np.prod(x, axis=-1, keepdims=True),  # REPLACE transform_fn=lambda x: ???\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machine\n",
    "\n",
    "Train an SVM using gradient descent. For which datasets does the algorithm converge?\n",
    "\n",
    "\\begin{align}\n",
    "\t\\tilde{\\mathcal{L}}(\\mathbf{w},w_0) = \\frac{\\|\\mathbf{w}\\|^2}{2} + \\frac C N \\sum_{n=1}^N \\max\\left(0, 1 - c_n(\\mathbf{w}^{\\mathsf T}\\mathbf{x}_n+w_0) \\right).\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Text from SML script to this equation (Eq. 4.36):\n",
    "\n",
    "> The first term will push the model to have a small weight vector $\\mathbf w$, leading to a large\n",
    "margin, while the second term computes the total of all margin violations. Minimizing\n",
    "this term ensures that the model makes the margin violations as small and as few as\n",
    "possible. $C$ is a trade-off parameter between the two contributions to the cost function.\n",
    "This objective function can be minimized by gradient descent.\n",
    ">\n",
    "> By the way, the function max(0, 1−x) is the hinge loss we have seen earlier in Section 2.7.3\n",
    "\n",
    "Apply the same feature transform as before to render the `circles` and `checkerboard` dataset linearly separable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are:\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial \\mathbf w} &= \\mathbf w + \\frac C N \\sum_{n=1}^N -c_n \\mathrm{heaviside}\\bigg(1 - c_n(\\mathbf{w}^{\\mathsf T}\\mathbf{x}_n+w_0)\\bigg) \\mathbf x_n \\\\\n",
    "\\frac{\\partial J}{\\partial w_0} &= \\frac C N \\sum_{n=1}^N -c_n \\mathrm{heaviside}\\bigg(1 - c_n(\\mathbf{w}^{\\mathsf T}\\mathbf{x}_n+w_0)\\bigg)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_support_vector_machine(dataset, transform_fn=None, iterations=10000,\n",
    "                               learning_rate=0.01, hinge_loss_weight=10):\n",
    "    loss_history = np.empty(iterations)\n",
    "    loss_history[:] = np.nan\n",
    "    N = dataset.features.shape[0]\n",
    "    x_tilde = add_augmentation(dataset.features)\n",
    "    \n",
    "    if transform_fn is not None:\n",
    "        x = remove_augmentation(x_tilde)\n",
    "        x = transform_fn(x)\n",
    "        x_tilde = add_augmentation(x)\n",
    "    \n",
    "    w_tilde = np.random.normal(size=x_tilde.shape[-1])\n",
    "    c = dataset.labels\n",
    "    for iteration in range(iterations):\n",
    "        discriminant, c_hat = predict_rosenblatt_perceptron(x_tilde, w_tilde)\n",
    "        z = 1 - c * discriminant  # REPLACE\n",
    "        loss_history[iteration] = np.linalg.norm(w_tilde[:-1]) / 2 + hinge_loss_weight * np.sum(np.maximum(0, z), axis=0)  # REPLACE loss_history[iteration] = ???\n",
    "        gradient_w = w_tilde[:-1] + hinge_loss_weight / N * np.einsum('n,n,nd->d', np.heaviside(z, 0.5), -c, x_tilde[:, :-1])  # REPLACE gradient_w = ???\n",
    "        gradient_w_0 = hinge_loss_weight / N * np.einsum('n,n->', np.heaviside(z, 0.5), -c)  # REPLACE gradient_w_0 = ???\n",
    "        gradient = np.concatenate((gradient_w, [gradient_w_0]))\n",
    "        w_tilde = w_tilde - learning_rate * gradient\n",
    "    return w_tilde, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(\n",
    "    hard_blobs,\n",
    "    fit_function=fit_support_vector_machine,\n",
    "    levels=[-1, 0, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(\n",
    "    easy_blobs,\n",
    "    fit_function=fit_support_vector_machine,\n",
    "    levels=[-1, 0, 1],\n",
    "    iterations=10000,  # REPLACE iterations=???,\n",
    "    hinge_loss_weight=100,  # REPLACE hinge_loss_weight=???,\n",
    "    learning_rate=0.01,  # REPLACE learning_rate=???,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(\n",
    "    circles,\n",
    "    fit_function=fit_support_vector_machine,\n",
    "    levels=[-1, 0, 1], \n",
    "    transform_fn=lambda x: np.linalg.norm(x, axis=-1, keepdims=True),  # REPLACE transform_fn=lambda x: ???,\n",
    "    hinge_loss_weight=1000  # REPLACE hinge_loss_weight=???\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(\n",
    "    checkerboard,\n",
    "    fit_function=fit_support_vector_machine,\n",
    "    transform_fn=lambda x: np.prod(x, axis=-1, keepdims=True),  # REPLACE transform_fn=lambda x: ???,\n",
    "    iterations=10000,  # REPLACE iterations=???,\n",
    "    learning_rate=0.001,  # REPLACE learning_rate=???,\n",
    "    hinge_loss_weight=10000,  # REPLACE hinge_loss_weight=???,\n",
    "    levels=[-1, 0, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_dataset = Dataset('sign', np.array([[1, 1], [-1, 1], [1, -1], [-1, -1]]) * 100, np.array([-1, -1, 1, 1]))\n",
    "plot_dataset(sign_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How should the decision boundary look like?\n",
    "- Can you explain, why the SVM code does not work for this simple example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(\n",
    "    sign_dataset,\n",
    "    fit_function=fit_support_vector_machine,\n",
    "    levels=[-1, 0, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_dataset2 = Dataset('sign2', np.array([[1, 1], [-1, 1], [0, -1], [0, -1.5]]), np.array([-1, -1, 1, 1]))\n",
    "plot_dataset(sign_dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How should the decision boundary look like?\n",
    "- Can you explain, why the decision boundary is not the expected one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot(\n",
    "    sign_dataset2,\n",
    "    fit_function=fit_support_vector_machine,\n",
    "    hinge_loss_weight=1,\n",
    "    levels=[-0.99, 0, 0.99],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the SVM from `sklearn` (https://scikit-learn.org/stable/index.html) to classify the datasets.\n",
    "\n",
    "The documentation is here\n",
    "- Theory: https://scikit-learn.org/stable/modules/svm.html#svc\n",
    "- Code: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "\n",
    "The `sklearn.svm.SVC` class implements a support vector machine for classification.\n",
    "- What algorithm does `SVC` implement? How does it differ from the one you implemented above?\n",
    "- Fit the SVM with the different kernels provided by sklearn. Which kernels work for which datasets?\n",
    "- Compare the results you obtained above with the ones obtained with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_decision_boundary(\n",
    "    dataset, title, svm, ax: plt.Axes=None\n",
    "):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"    \n",
    "    show = False\n",
    "    if ax is None:\n",
    "        show = True\n",
    "        _, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    # Plot decision boundary if in area\n",
    "    steps = 100\n",
    "    x, y, features_grid = compute_complete_grid(\n",
    "        (np.min(dataset.features[:, 0]), np.max(dataset.features[:, 0])),\n",
    "        (np.min(dataset.features[:, 1]), np.max(dataset.features[:, 1])),\n",
    "        steps=steps\n",
    "    )\n",
    "    \n",
    "    features_grid = features_grid.reshape(steps * steps, 2)\n",
    "    z = svm.predict(features_grid)\n",
    "    z = np.reshape(z, (steps, steps))\n",
    "    \n",
    "    ax.contourf(x, y, z, colors=['cyan', 'yellow'], alpha=0.2)\n",
    "    plot_dataset(dataset, ax=ax, title=title)\n",
    "    ax.contour(x, y, z, levels=[0])\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_plot_svm(dataset, kernel='linear', ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    ??? Add a doc string. What does this function do? What are the input parameters? Which shape?\n",
    "    \"\"\"\n",
    "    svm = SVC(kernel=kernel, **kwargs) # REPLACE svm = ???\n",
    "    svm.fit(dataset.features, dataset.labels) # REPLACE \n",
    "    \n",
    "    plot_svm_decision_boundary(dataset, title=f'{dataset.name} {kernel}', svm=svm, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "for i, dataset in enumerate((circles, easy_blobs, checkerboard, hard_blobs)):\n",
    "    for j, kernel in enumerate(('linear', 'poly', 'sigmoid', 'rbf')):\n",
    "        fit_and_plot_svm(dataset, kernel=kernel, ax=axes[i,j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit the SVM with the transform functions you used above. \n",
    "- Sklearn takes a kernel function instead of a transform function\n",
    "- The kernel function takes feature matrices `d1` and `d2` instead of a single data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kernel_matrix_circles(d1, d2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        d1: First data matrix, shape (n_samples, n_features)\n",
    "        d2: Second data matrix, shape (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        kernel_matrix, shape (n_samples, n_samples)\n",
    "    \"\"\"\n",
    "    x1 = np.sum(d1**2, axis=-1) # REPLACE\n",
    "    x2 = np.sum(d2**2, axis=-1) # REPLACE\n",
    "    kernel_matrix = x1[:, None] * x2[None, :]  # REPLACE kernel_matrix = ???\n",
    "    return kernel_matrix\n",
    "fit_and_plot_svm(circles, kernel=compute_kernel_matrix_circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kernel_matrix_checkerboard(d1, d2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        d1: First data matrix, shape (n_samples, n_features)\n",
    "        d2: Second data matrix, shape (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        kernel_matrix, shape (n_samples, n_samples)\n",
    "    \"\"\"\n",
    "    x1 = np.prod(d1, axis=-1) # REPLACE\n",
    "    x2 = np.prod(d2, axis=-1) # REPLACE\n",
    "    kernel_matrix = x1[:, None] * x2[None, :] # REPLACE kernel_matrix = ???\n",
    "    return kernel_matrix\n",
    "fit_and_plot_svm(checkerboard, kernel=compute_kernel_matrix_checkerboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_dataset3 = Dataset('sign3', np.array([[1, 1], [-1, 1], [0, -1], [0, -1.5]])/100, np.array([-1, -1, 1, 1]))\n",
    "plot_dataset(sign_dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How should the decision boundary look like?\n",
    "- Can you explain, why the decision boundary is not the expected one?\n",
    "- Can you find hyperparameters so that the SVM finds the expected decision boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot_svm(sign_dataset3, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot_svm(sign_dataset3, kernel='linear', C=10000)  # REPLACE fit_and_plot_svm(sign_dataset3, kernel='linear', ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For experts...\n",
    "\n",
    "- Mark the support vectors in your diagrams.\n",
    "- What is the advantage of training the SVM with gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
