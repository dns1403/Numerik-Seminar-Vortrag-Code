{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import sklearn.datasets\n",
    "from scipy.stats import multivariate_normal\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "\n",
    "import sys\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = True  # Use per default a grid, i.e. plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Tab`: Code completion or indent\n",
    "- `Shift-Tab`: Tooltip\n",
    "- `Shift-Tab Shift+Tab`: Long tooltip\n",
    "- `Ctrl-Enter`: Run selected cells\n",
    "- `Shift-Enter`: Run cell, select below\n",
    "- `Alt-Enter`: Run cell and insert below\n",
    "- `Ctrl-#`: Comment\n",
    "- `Esc a`: New cell above\n",
    "- `Esc b`: New cell below\n",
    "- `Esc d d`: Delete current cell\n",
    "- `Esc m`: Turn current cell into Markdown cell\n",
    "- `Esc y`: Turn current cell into code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1*f6KbPXwksAliMIsibFyGJw.png](https://miro.medium.com/max/700/1*f6KbPXwksAliMIsibFyGJw.png)\n",
    "\n",
    "<!-- Source https://towardsdatascience.com/the-iris-dataset-a-little-bit-of-history-and-biology-fb4812f5a7b5 -->\n",
    " \n",
    "\n",
    "| English | Description | German\n",
    "|:--- |:--- |:---\n",
    "|Iris | flowering plant genus | Schwertlilien\n",
    "|Iris setosa | iris type | Borsten-Schwertlilie\n",
    "|Iris versicolor | iris type | Verschiedenfarbige Schwertlilie\n",
    "|Iris virginica | iris type | Virginische Schwertlilie\n",
    "|Sepal | Leaf type | Kelchblatt\n",
    "|Petal | Leaf type | BlÃ¼tenblatt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sklearn.datasets.load_iris()\n",
    "print(ds['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_scatter_to_plot(ds):\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    for target_index, (target_name, color) in enumerate(zip(ds['target_names'], colors)):\n",
    "        X = ds['data'][ds['target'] == target_index, :]\n",
    "        plt.scatter(X[:, 2], X[:, 3], c=color, label=target_name)\n",
    "    plt.legend()\n",
    "    plt.xlabel(ds['feature_names'][2])\n",
    "    plt.ylabel(ds['feature_names'][3])\n",
    "\n",
    "add_scatter_to_plot(ds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Two-class Logistic Regression with Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior probabilities of a two class problem (here: \"virginica\" vs. \"not virginica\") shall be estimated using logistic regression.\n",
    "- Start with the logistic regression loglikelihood function (script Eq. (2.43) and derive a gradient based iterative update scheme for the parameters $\\mathbf w$ and $w_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Define and plot the sigmoid function\n",
    "The sigmoid function is defined as follows:\n",
    "\\begin{align}\n",
    "\\sigma(x) = \\frac{1}{1+\\mathrm e^{-x}} &= (1+\\mathrm e^{-x})^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return ???\n",
    "\n",
    "x = np.linspace(-10, 10, num=100)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Find the update equations for gradient ascent\n",
    "\n",
    "The log-likelihood is defined as follows (see lecture):\n",
    "\n",
    "\\begin{align}\n",
    "\\ell( w_0, \\mathbf w) &= \\sum_{n=1}^N \\bigg(\n",
    "\\tilde c_n \\ln \\, \\sigma(\\mathbf w^{\\mathsf T}\\mathbf x_n + w_0) + (1-\\tilde c_n)\\ln\\,\\big(1-\\sigma(\\mathbf w^{\\mathsf T}\\mathbf x_n + w_0)\\big)\n",
    "\\bigg) \\\\\n",
    "\\tilde c_n &\\in \\{0, 1\\}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf w = [w_1, w_2, ...]^{\\mathsf T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the matter, lets first find a nice derivative for the sigmoid function.\n",
    "\n",
    "**Find a form of the derivative of $\\sigma(x)$ which just depends on $\\sigma(x)$ and _not_ directly on $e^{()}$ or $\\sigma(-x)$**\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\sigma(x)}{\\partial x}\n",
    "&= \\color{red}{???}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the derivatives of the cost function itself.**\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial\\ell}{\\partial w_0}\n",
    "&= \\color{red}{???}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial\\ell}{\\partial w_d}\n",
    "&= \\color{red}{???}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formulate the gradient ascent algorithm.**\n",
    "\\begin{align}\n",
    "\\mathbf w^{(\\kappa + 1)}\n",
    "&= \\color{red}{???} &\n",
    "w_0^{(\\kappa + 1)}\n",
    "&= \\color{red}{???}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Implement the gradient ascent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define `features` to only use the petal length and width and `labels` for the two classes \"virginica\" and \"not virginica\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ds['data'][:, 2:]\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (ds['target'] == 2)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for implementing $\\mathbf w^{\\mathsf T}\\mathbf x_n$**\n",
    "\n",
    "We need to multiply the `features` and weight vector `w` along the feature dimension and then perform the summation. This is what corresponds to $\\mathbf w^{\\mathsf T}\\mathbf x_n$ in math notation.\n",
    "\n",
    "Possible ways:\n",
    "\n",
    "- `np.sum(w[None, :] * features, axis=-1)`: Here, `None` introduces a singleton dimension in `w`. Then elementwise multiplication uses broadcasting.\n",
    "- `features @ w`: Multiplies the last dimension of `features` with the first dimension of `w`.\n",
    "- `einsum('nd,d->n', features, w)`: Here, since `d` appears more than once, multiplication and summation is performed along that axis. Check the [Einsum documentation](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.einsum.html).\n",
    "\n",
    "All presented ways use low-level C or Fortran code to perform the loop iterations.\n",
    "Decide for yourself, which one genereralizes well and stays readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the algorithm in the template below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "w = np.zeros((D,))\n",
    "w0 = np.zeros(())\n",
    "\n",
    "# iterations: number of update steps\n",
    "iterations = ???\n",
    "learning_rate = ???\n",
    "for _ in range(iterations):\n",
    "    grad_w0 = ???\n",
    "    grad_w = ???\n",
    "    \n",
    "    assert grad_w0.shape == (), grad_w0.shape\n",
    "    assert grad_w.shape == (D,), grad_w.shape\n",
    "    \n",
    "    # Gradient ascent, since we want to maximize the log-likelihood.\n",
    "    w0 = ???\n",
    "    w = ???\n",
    "    \n",
    "    assert w.shape == (D,), w.shape\n",
    "    assert w0.shape == (), w0.shape\n",
    "\n",
    "w, w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_complete_grid(xlim, ylim, steps):\n",
    "    x, y = np.meshgrid(\n",
    "        np.linspace(*xlim, steps),\n",
    "        np.linspace(*ylim, steps)\n",
    "    )\n",
    "    features_grid = np.moveaxis(np.array([x, y]), source=0, destination=-1)\n",
    "    assert features_grid.shape == (steps, steps, 2), features_grid.shape\n",
    "    return x, y, features_grid\n",
    "\n",
    "steps = 100\n",
    "x, y, features_grid = compute_complete_grid((0, 7), (0, 3), steps=steps)\n",
    "\n",
    "z = sigmoid(features_grid @ w + w0)\n",
    "\n",
    "plt.contourf(x, y, z)\n",
    "plt.clim(0, 1)\n",
    "plt.colorbar(label='Estimated posterior for virginica')\n",
    "\n",
    "add_scatter_to_plot(ds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# b) Two-class Linear Discriminant Analysis (LDA)\n",
    "\n",
    "Now, we assume that the distribution of each class \"not virginica\" vs. \"virginica\" is Gaussian.\n",
    "- Estimate the mean vector and covariance matrix for each class.\n",
    "  - For individual covariance matrices.\n",
    "  - For tied covariance matrices.\n",
    "- Plot the data and the corresponding covariance contours (1-sigma environments).\n",
    "  - For individual covariance matrices.\n",
    "  - For tied covariance matrices.\n",
    "- Calculate the linear discriminant (LDA) for classification.\n",
    "  - For tied covariance matrices.\n",
    "  - Check, what happens if you ignore the prior weights. In what direction does the decision boundary move?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Linear Discriminant Analysis (LDA)\n",
    "\n",
    "$p({\\bf x} | \\omega_k) = \\mathcal N({\\bf x}; {\\boldsymbol \\mu_k} , {\\bf \\Sigma})$\n",
    "\n",
    "### Discriminant\n",
    "\n",
    "$g_k({\\bf x}) = \\ln p({\\bf x}, \\omega_k)$\n",
    "\n",
    "### Decision Rule\n",
    "\n",
    "$\\hat \\omega = \\underset{\\omega_k}{\\operatorname{argmax}} \\{g_k({\\bf x})\\}$\n",
    "\n",
    "$\\dots$ (Same as first exercise about Bayes Decision rule, when considering only 2 classes)\n",
    "\n",
    "#### Solution for 2 classes\n",
    "\n",
    "${\\bf w} = {\\bf \\Sigma}^{-1} (\\boldsymbol \\mu_1 - \\boldsymbol \\mu_2)$\n",
    "\n",
    "$\\mathbf x_0 = \\frac{1}{2} (\\boldsymbol \\mu_1 + \\boldsymbol\\mu_2) - \\dfrac{\\ln\\left(\\dfrac{\\operatorname{Pr}(\\omega_1)}{\\operatorname{Pr}(\\omega_2)}\\right)}{(\\boldsymbol \\mu_1 - \\boldsymbol\\mu_2)^{\\mathrm T} \\boldsymbol \\Sigma^{-1} (\\boldsymbol \\mu_1 - \\boldsymbol\\mu_2)}(\\boldsymbol \\mu_1 - \\boldsymbol\\mu_2)$\n",
    "\n",
    "$\\mathbf{w}^T(\\mathbf{x} - \\mathbf x_0)  \\overset{\\hat{\\omega}=\\omega_1}{\\underset{\\hat{\\omega}=\\omega_2}{\\gtrless}} 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ds['data'][:, 2:]\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (ds['target'] == 2).astype(int)\n",
    "print(labels.shape)\n",
    "labels[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[labels == 0, :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have different ways to calculate the mean and covariance.\n",
    "- We can either select the data first or provide weights to the mean function.\n",
    "- Here, we start with the simple version, where we select the data outside of the function.\n",
    "- A bias correction for the covariance matrix is ignored here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the mean and covariance estimation functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(x):\n",
    "    \"\"\"Computes the empirical mean of `x`\"\"\"\n",
    "    return np.mean(???)\n",
    "\n",
    "def covariance(x):\n",
    "    \"\"\"Computes the empirical covariance of `x`\"\"\"\n",
    "    ???\n",
    "    ???\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write a small test.\n",
    "From the dataset description, we know the global mean and standard deviation of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('    =' + ds['DESCR'].split('=', maxsplit=1)[1].rsplit('=', maxsplit=1)[0] + '=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_almost_equal(actual, desired, atol=0.01):\n",
    "    \"\"\"Allow for some variation due to numerical issues\"\"\"\n",
    "    if not np.all(np.abs(actual - desired) < atol):\n",
    "        raise AssertionError(\n",
    "            f'Difference is too big:\\n'\n",
    "            f'{actual} - {desired} = {actual - desired}'\n",
    "        )\n",
    "\n",
    "global_mean = mean(features)\n",
    "\n",
    "assert global_mean.shape == (D,), global_mean.shape\n",
    "test_almost_equal(global_mean[0], 3.76)\n",
    "test_almost_equal(global_mean[1], 1.20)\n",
    "\n",
    "global_covariance = covariance(features)\n",
    "\n",
    "assert global_covariance.shape == (D, D), global_covariance.shape\n",
    "test_almost_equal(global_covariance[0, 0], 1.76 ** 2)\n",
    "test_almost_equal(global_covariance[1, 1], 0.76 ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Different mean and different covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the mean and covariance matrices for both classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_not_virginica = ???\n",
    "mean_virginica = ???\n",
    "covariance_not_virginica = ???\n",
    "covariance_virginica = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_not_virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we help you with some tests, where we compare your estimates with the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_almost_equal(mean_not_virginica, [2.861, 0.786])\n",
    "test_almost_equal(mean_virginica, [5.552, 2.026])\n",
    "test_almost_equal(covariance_not_virginica, [[2.080179, 0.794254], [0.794254, 0.316204]])\n",
    "test_almost_equal(covariance_virginica, [[0.298496, 0.047848], [0.047848, 0.073924]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \"not_virginica\" covariance contour (so called 1-sigma environment).\n",
    "z_not_virginica = multivariate_normal.pdf(features_grid, mean_not_virginica, covariance_not_virginica)\n",
    "levels = [multivariate_normal.pdf(np.zeros((D,)), np.zeros((D,)), covariance_not_virginica) * np.exp(-0.5 * 1 ** 2)]\n",
    "plt.contour(x, y, z_not_virginica, levels)\n",
    "\n",
    "# Plot \"virginica\" covariance contour (so called 1-sigma environment).\n",
    "z_virginica = multivariate_normal.pdf(features_grid, mean_virginica, covariance_virginica)\n",
    "levels = [multivariate_normal.pdf(np.zeros((D,)), np.zeros((D,)), covariance_virginica) * np.exp(-0.5 * 1 ** 2)]\n",
    "plt.contour(x, y, z_virginica, levels)\n",
    "\n",
    "add_scatter_to_plot(ds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes virginica and not virginica have now different covariance matrices, hence the decision boundary will be nonlinear.\n",
    "\n",
    "**Use a numeric approach to find the decision boundary**\n",
    " - Calculate the discriminant $g$ for the two class problem: $g(\\mathbf{x}) = g_1(\\mathbf{x}) - g_2(\\mathbf{x})$ for each point in a grid.\n",
    "     - Hint: `z_virginica` and `z_not_virginica` are the values for $p(\\mathbf{x}|\\omega_k)$ for a grid.\n",
    "       You can use them as starting point to calculate the discriminant\n",
    " - Calculate the estimated class for each grid point: $g(\\mathbf{x})  \\overset{\\hat{\\omega}=\\omega_1}{\\underset{\\hat{\\omega}=\\omega_2}{\\gtrless}} 0$ (Code provided)\n",
    " - Visualize the estimated classes (Code provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = ???\n",
    "\n",
    "# Plot the estimated classes with the help of a contour plot\n",
    "plt.contourf(x, y, 0 > g)\n",
    "plt.colorbar(label='1 decide for virginica, 2 decide for not virginica')\n",
    "\n",
    "add_scatter_to_plot(ds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the estimator may be more precise using different covariance matrices, it will not yield a linear discriminant.\n",
    "Therefore, we here simplify the model further to just allow a shared covariance matrix (tied covariance matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Different mean but tied covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate a tied covariance, we need to calculate the mean per class first.\n",
    "We can then subtract the mean of the corresponding class from each feature.\n",
    "\n",
    "This is a bit cumbersome.\n",
    "We may also calculate the mean per class and weight it with the count per class $N_k$.\n",
    "\n",
    "Please note, that we again ignore any bias correction.\n",
    "\n",
    "The most robust recipe is as follows ([Scikit-Learn](https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/mixture/gaussian_mixture.py#L172)):\n",
    "\\begin{align}\n",
    "\\boldsymbol\\Sigma = \\frac 1 N \\sum_{n=1}^N \\mathbf x_n \\mathbf x_n^{\\mathsf T} - \\frac{1}{N} \\sum_{k=1}^K N_k \\boldsymbol \\mu_k \\boldsymbol \\mu_k^{\\mathsf T}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_not_virginica = ???\n",
    "N_virginica = ???\n",
    "N = ???\n",
    "tied_covariance = (\n",
    "    ???\n",
    "    ???\n",
    "    ???\n",
    ")\n",
    "tied_covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_almost_equal(tied_covariance, [[1.48628467, 0.545452], [0.545452, 0.235444]])  # Test vs solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \"not_virginica\" covariance contour (so called 1-sigma environment).\n",
    "z = multivariate_normal.pdf(features_grid, mean_not_virginica, tied_covariance)\n",
    "plt.contour(x, y, z, [\n",
    "    multivariate_normal.pdf(np.zeros((D,)), np.zeros((D,)), tied_covariance) * np.exp(-0.5 * 1 ** 2)\n",
    "])\n",
    "\n",
    "# Plot \"virginica\" covariance contour (so called 1-sigma environment).\n",
    "z = multivariate_normal.pdf(features_grid, mean_virginica, tied_covariance)\n",
    "plt.contour(x, y, z, [\n",
    "    multivariate_normal.pdf(np.zeros((D,)), np.zeros((D,)), tied_covariance) * np.exp(-0.5 * 1 ** 2)\n",
    "])\n",
    "\n",
    "add_scatter_to_plot(ds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_lda = ???\n",
    "x0_lda = ???\n",
    "w_lda, x0_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_almost_equal(w_lda, [0.81613039, -7.15737056])  # Test vs solution\n",
    "test_almost_equal(x0_lda, [4.48577502, 1.5346886])  # Test vs solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \"not_virginica\" covariance contour (so called 1-sigma environment).\n",
    "z = multivariate_normal.pdf(features_grid, mean_not_virginica, tied_covariance)\n",
    "plt.contour(x, y, z, [\n",
    "    multivariate_normal.pdf(np.zeros((D,)), np.zeros((D,)), tied_covariance) * np.exp(-0.5 * 1 ** 2)\n",
    "])\n",
    "\n",
    "# Plot \"virginica\" covariance contour (so called 1-sigma environment).\n",
    "z = multivariate_normal.pdf(features_grid, mean_virginica, tied_covariance)\n",
    "plt.contour(x, y, z, [\n",
    "    multivariate_normal.pdf(np.zeros((D,)), np.zeros((D,)), tied_covariance) * np.exp(-0.5 * 1 ** 2)\n",
    "])\n",
    "\n",
    "x_linspace = np.linspace(0, 7, 2)\n",
    "plt.plot(x_linspace, -w_lda[0] / w_lda[1] * x_linspace + w_lda[0] / w_lda[1] * x0_lda[0] + x0_lda[1])\n",
    "\n",
    "add_scatter_to_plot(ds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Three-class logistic regression: softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume more than two cases, the data follows no longer a bernoulli distribution, but a multinoulli distribution.\n",
    "The likelihood is thus a product of multinoulli distributions:\n",
    "\n",
    "\\begin{align}\n",
    "L &= \\prod_{n=1}^{N} \\prod_{k=1}^{K} p_k^{c_{n,k}}\n",
    "\\end{align}\n",
    "\n",
    "From this follows that $p_k$ is modeled with a the softmax (see lecture / script chapter 2.6.1, equation (2.44)). The softmax is defined as (for a scalar)\n",
    "\\begin{align}\n",
    "    \\text{softmax}(x_k) = \\frac{\\exp(x_k)}{\\sum_{k'}\\exp(x_{k'})}\n",
    "\\end{align}\n",
    "\n",
    "The log-likelihood is\n",
    "\n",
    "\\begin{align}\n",
    "\\ell &= \\sum_{n=1}^{N} \\sum_{k=1}^{K} c_{n,k} \\ln (\\text{softmax}(\\mathbf w_k^{\\mathsf T} \\mathbf x_n + w_{0,k})) \\\\\n",
    "&= \\sum_{n=1}^{N} \\sum_{k=1}^{K} c_{n,k} \\ln \\frac{\n",
    "\\mathrm e^{\\mathbf w_k^{\\mathsf T} \\mathbf x_n + w_{0,k}}\n",
    "}{\n",
    "\\sum_{k'=1}^K \\mathrm e^{\\mathbf w_{k'}^{\\mathsf T} \\mathbf x_n + w_{0,k'}}\n",
    "} \\\\\n",
    "&= \\sum_{n=1}^{N} \\sum_{k=1}^{K} c_{n,k} \\ln \\left(\\mathrm e^{\\mathbf w_k^{\\mathsf T} \\mathbf x_n + w_{0,k}}\\right)\n",
    "- \\sum_{n=1}^{N} \\sum_{k=1}^{K} c_{n,k} c_{n,k} \\ln \\left(\\sum_{k'=1}^K \\mathrm e^{\\mathbf w_{k'}^{\\mathsf T} \\mathbf x_n + w_{0,k'}}\\right) \\\\\n",
    "&= \\sum_{n=1}^{N} \\sum_{k=1}^{K} c_{n,k} \\left(\\mathbf w_k^{\\mathsf T} \\mathbf x_n + w_{0,k}\\right)\n",
    "- \\sum_{n=1}^{N} \\ln \\left(\\sum_{k'=1}^K \\mathrm e^{\\mathbf w_{k'}^{\\mathsf T} \\mathbf x_n + w_{0,k'}}\\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "And the derivatives w.r.t. the parameters\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ell}{\\partial w_{0,k}} &= \\sum_{n=1}^{N} \\left( c_{n,k} -\\frac{\n",
    "\\mathrm e^{\\mathbf w_{k'}^{\\mathsf T} \\mathbf x_n + w_{0,k'}}\n",
    "}{\n",
    "\\sum_{k'=1}^K \\mathrm e^{\\mathbf w_{k'}^{\\mathsf T} \\mathbf x_n + w_{0,k'}}\n",
    "} \\right) \\\\\n",
    "\\frac{\\partial \\ell}{\\partial w_{k,d}} &= \\sum_{n=1}^{N} \\left( c_{n,k}x_{n,d} -\\frac{\n",
    "\\mathrm e^{\\mathbf w_{k'}^{\\mathsf T} \\mathbf x_n + w_{0,k'}}\n",
    "}{\n",
    "\\sum_{k'=1}^K \\mathrm e^{\\mathbf w_{k'}^{\\mathsf T} \\mathbf x_n + w_{0,k'}}\n",
    "}x_{n,d} \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the softmax function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.random.normal(size=(3, 4))\n",
    "actual = np.sum(softmax(test_data), axis=-1)\n",
    "desired = np.ones((3,))\n",
    "np.testing.assert_allclose(actual, desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ds['data'][:, 2:]\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_one_hot(labels, num_classes=None):\n",
    "    \"\"\"Experiment with inputs to this function to check what it does.\"\"\"\n",
    "    num_classes = np.max(labels) + 1 if num_classes is None else num_classes\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "labels_one_hot = labels_to_one_hot(ds['target'])\n",
    "print(labels_one_hot.shape)\n",
    "labels_one_hot[123, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "actual = labels_to_one_hot([2], num_classes=4)\n",
    "desired = [0, 0, 1, 0]\n",
    "assert np.all(np.equal(actual, desired))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now state an alternative relation between `softmax` and `argmax`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax([10, 20, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax([10, 20, 30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_one_hot(np.argmax([10, 20, 30]), num_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement the gradient ascent algorithm for the three-class softmax regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "K = 3\n",
    "w = np.zeros((D, K))\n",
    "w0 = np.zeros((K,))\n",
    "\n",
    "iterations = ???\n",
    "learning_rate = ???\n",
    "for _ in range(iterations):\n",
    "    grad_w0 = ???\n",
    "    grad_w = ???\n",
    "    \n",
    "    assert grad_w0.shape == (K,), grad_w0.shape\n",
    "    assert grad_w.shape == (D, K), grad_w.shape\n",
    "    \n",
    "    # Gradient ascent, since we want to maximize the log-likelihood.\n",
    "    w0 = ???\n",
    "    w = ???\n",
    "    \n",
    "    assert w0.shape == (K,), w0.shape\n",
    "    assert w.shape == (D, K), w.shape\n",
    "\n",
    "w, w0, grad_w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 100\n",
    "x, y, features_grid = compute_complete_grid((0, 7), (0, 3), steps=steps)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for k, ax in enumerate(axes):\n",
    "    z = softmax(features_grid @ w + w0)[..., k]\n",
    "    ax.contourf(x, y, z)\n",
    "    ax.set_title('Estimated posterior for {}'.format(ds['target_names'][k]))\n",
    "\n",
    "    for target_index, (target_name, color) in enumerate(zip(ds['target_names'], ['blue', 'red', 'green'])):\n",
    "        X = ds['data'][ds['target'] == target_index, :]\n",
    "        ax.scatter(X[:, 2], X[:, 3], c=color, label=target_name)\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(ds['feature_names'][2])\n",
    "    ax.set_ylabel(ds['feature_names'][3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d) Three-class LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform an LDA for the three class problem and visualize the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ds['data'][:, 2:]\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_one_hot(labels, num_classes=None):\n",
    "    \"\"\"Experiment with inputs to this function, to check what it does.\"\"\"\n",
    "    num_classes = np.max(labels) + 1 if num_classes is None else num_classes\n",
    "    return np.eye(num_classes)[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels_to_one_hot(ds['target'])\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = features.shape[0]\n",
    "counts = ???\n",
    "means = ???\n",
    "tied_covariance = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3  # Number of classes\n",
    "\n",
    "for k in range(K):\n",
    "    z = multivariate_normal.pdf(features_grid, means[k, :], tied_covariance)\n",
    "    \n",
    "    levels = [\n",
    "        multivariate_normal.pdf(np.zeros((D,)), np.zeros((D,)), tied_covariance) * np.exp(-0.5 * 1 ** 2)\n",
    "    ]\n",
    "    plt.contour(x, y, z, levels)\n",
    "    \n",
    "z = np.stack([multivariate_normal.pdf(features_grid, means[k, :], tied_covariance) for k in range(K)])\n",
    "c_hat = np.argmax(z, axis=0)\n",
    "\n",
    "plt.contourf(x, y, c_hat, alpha=0.2)\n",
    "\n",
    "add_scatter_to_plot(ds)\n",
    "    \n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(ds['feature_names'][2])\n",
    "plt.ylabel(ds['feature_names'][3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e) Apply the MAP criterion and calculate accuracy for each 3-class estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the MAP criterion to classify the data with both learned classifiers. Calculate the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hat = ???\n",
    "c_hat = ???\n",
    "correct = np.sum(np.equal(c_hat, ds['target']))\n",
    "N = len(ds['target'])\n",
    "print('Accuracy: {} %'.format(correct / N * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ???\n",
    "c_hat = ???\n",
    "correct = np.sum(np.equal(c_hat, ds['target']))\n",
    "N = len(ds['target'])\n",
    "print('Accuracy: {} %'.format(correct / N * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlook: Pandas DataFrame\n",
    "Pandas DataFrame provides a table-based view on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    ds.data,\n",
    "    columns=ds.feature_names,\n",
    ")\n",
    "df = pd.concat([df, pd.DataFrame([ds.target_names[t] for t in ds.target], columns=['target_names'])], axis=1)\n",
    "seaborn.scatterplot(data=df, x='petal length (cm)', y='petal width (cm)', hue='target_names')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For experts...\n",
    "\n",
    "You can also `from sklearn.linear_model import LogisticRegression` and apply it to our dataset.\n",
    "It has way more parameters than our algorithm. Check especially, what the control parameter `C` does [1].\n",
    "Can you spot at least the code [2], which performs the prediction?\n",
    "\n",
    "- [1] http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- [2] https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/linear_model/logistic.py#L962"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further reading\n",
    "- Logistic regression for a related problem: http://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "- How are tied covariance matrices enforced? https://stats.stackexchange.com/questions/175477/how-is-the-tied-covariance-matrix-enforced-in-linear-discriminant-analysis\n",
    "- Softmax regression tutorial: http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
