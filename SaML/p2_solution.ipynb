{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least squares with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import hashlib\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['axes.grid'] = True  # Use per default a grid, i.e. plt.grid()\n",
    "# mpl.rcParams['figure.figsize'] = [6.4, 4.8]  # Change the default figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Tab`: Code completion or indent\n",
    "- `Shift-Tab`: Tooltip\n",
    "- `Shift-Tab Shift+Tab`: Long tooltip\n",
    "- `Ctrl-Enter`: Run selected cells\n",
    "- `Shift-Enter`: Run cell, select below\n",
    "- `Alt-Enter`: Run cell and insert below\n",
    "- `Ctrl-#`: Comment\n",
    "- `Esc a`: New cell above\n",
    "- `Esc b`: New cell below\n",
    "- `Esc d d`: Delete current cell\n",
    "- `Esc m`: Turn current cell into Markdown cell\n",
    "- `Esc y`: Turn current cell into code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the California household prices dataset\n",
    "\n",
    "- Each row contains data about a block group (600 to 3000 people)\n",
    "- Some values are missing/ clipped or scaled.\n",
    "\n",
    "Values in the dataset:\n",
    "1. longitude: A measure of how far west a house is; a higher value is farther west\n",
    "2. latitude: A measure of how far north a house is; a higher value is farther north\n",
    "3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
    "4. totalRooms: Total number of rooms within a block\n",
    "5. totalBedrooms: Total number of bedrooms within a block\n",
    "6. population: Total number of people residing within a block\n",
    "7. households: Total number of households, a group of people residing within a home unit, for a block\n",
    "8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    "9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n",
    "10. oceanProximity: Location of the house w.r.t ocean/sea\n",
    "\n",
    "Inspect the dataset and discuss what the `pairplot` shows.\n",
    "Does it help to decide, which features might be good to predict `median_house_value`?\n",
    "\n",
    "For those that use `cocalc.com`:\n",
    "If you do not have internet access in your Jupyter notebook, you need to download the `housing.tgz` manually and then upload it to your environment just as you uploaded the notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path('housing.csv')\n",
    "\n",
    "if not csv_path.is_file():\n",
    "    remote_url = 'https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz'\n",
    "    local_path = Path('housing.tgz')\n",
    "\n",
    "    if not local_path.is_file():\n",
    "        urllib.request.urlretrieve(remote_url, local_path)\n",
    "\n",
    "    with tarfile.open(local_path) as f:\n",
    "        f.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check https://seaborn.pydata.org/examples/index.html\n",
    "sns.pairplot(df.dropna(), diag_kws={'bins': 50});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove missing data and clipped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When data is missing or is corrupted, you basically have three options:\n",
    "- Drop the row (i.e. the record containing the missing value)\n",
    "- Drop the column (i.e. drop the complete feature)\n",
    "- Fill with a sane default value (often median of the train set, has to be stored for the test conditions later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops all rows which contain NaN anywhere\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(6.4 * 2, 4.8))\n",
    "for ax, feature in zip(axes, ['median_house_value', 'housing_median_age']):\n",
    "    ax.hist(df[feature], bins=50)\n",
    "    ax.set_title(feature)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "Are there corrupted values?\n",
    " \n",
    " - Do you have an idea, why they are corrupted?\n",
    " - How can you identify them?\n",
    " - Check https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['median_house_value'].max(), df['housing_median_age'].max()  # REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.query('median_house_value < 500001')  # REPLACE df = df.query(???)\n",
    "df = df.query('housing_median_age < 52')  # REPLACE df = df.query(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(6.4*2, 4.8))\n",
    "for ax, feature in zip(axes, ['median_house_value', 'housing_median_age']):\n",
    "    ax.hist(df[feature], bins=50)\n",
    "    ax.set_title(feature)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split dataset into subsets\n",
    "\n",
    "It is absolutely crutial to split the dataset into at least two subsets:\n",
    "- Train set\n",
    "- Test set\n",
    "\n",
    "Often, people tend to split the dataset into three different subsets:\n",
    "- Train set\n",
    "- Cross-validation set/ Validation set\n",
    "- Test set\n",
    "\n",
    "You would then train on the train set and always check how your algorithm performs on the CV set.\n",
    "Just for the final results you would run your model once on the test set but not change any parameters afterwards anymore.\n",
    "The best model has to be selected based on the CV set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['_id'] = df.apply(lambda row: '{}_{}'.format(\n",
    "    row[\"longitude\"],\n",
    "    row[\"latitude\"]\n",
    "), axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the dataset into a train and test set.\n",
    "\n",
    "### Questions:\n",
    "\n",
    " - Do you get a random test set?\n",
    " - Is the test set each time the same set?\n",
    " - What would happen, when we get new samples or decide to drop more sample? Would the train/test assignment change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_test_set(key, test_ratio):\n",
    "    hash_value = hashlib.md5(key.encode()).digest()\n",
    "    return hash_value[-1] < 256 * test_ratio\n",
    "\n",
    "def split_train_test(dataframe, test_ratio):\n",
    "    in_test_set = dataframe._id.apply(lambda _id: is_in_test_set(_id, test_ratio))\n",
    "    return dataframe.loc[~in_test_set], dataframe.loc[in_test_set]\n",
    "\n",
    "df_train, df_test = split_train_test(df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df:       {}'.format(len(df)))\n",
    "print('df_train: {}'.format(len(df_train)))\n",
    "print('df_test:  {}'.format(len(df_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model the median house value based on median income\n",
    "\n",
    "Fit the ordinary least squares model to predict median house values based on median income.\n",
    "\n",
    "$y_n = w_0 + w_1 x_n + v_n$ with $v_n \\sim \\mathcal{N}(0, \\sigma_v^2)$\n",
    "\n",
    "### Matrix notation:\n",
    "The objective can be written as $J =  \\lVert\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}\\rVert^2$ with \n",
    "\n",
    " - Observation matrix: $\\mathbf{X} = \\begin{bmatrix} 1 & x_{1,1} & \\dots & x_{1,D-1} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{N,1} & \\dots & x_{N,D-1} \\end{bmatrix} \\in \\mathbb{R}^{N \\times (D)} = \\mathbb{R}^{14744 \\times 2}$\n",
    " - Target vector $\\mathbf{y} = \\begin{bmatrix}y_1\\\\\\vdots\\\\y_N\\end{bmatrix}$\n",
    " - Parameter vector $\\boldsymbol{\\theta} = \\begin{bmatrix}w_0\\\\\\vdots\\\\w_{D-1}\\end{bmatrix}$\n",
    " \n",
    "After some simple steps (Try it, it is important to be able to optimize such an objective) we obtain $\\boldsymbol{\\theta} = (\\mathbf{X}^\\mathrm{T} \\mathbf{X})^{-1} \\mathbf{X}^\\mathrm{T}\\mathbf{y}$\n",
    "\n",
    "### Questions:\n",
    " - The start equation is a polynom with order 1.\n",
    "   It is easy to see, that this is a linear classifier.\n",
    "   When we increase the polynom order $y_n = w_0 + w_1 x_n + \\dots + w_{D-1} x_n^{D-1} + v_n$, would it be still a linear classifier?\n",
    " - Why is there a column of ones in the observation matrix?\n",
    " - Does $(\\mathbf{X}^\\mathrm{T} \\mathbf{X})^{-1}$ exist? How about $(\\mathbf{X} \\mathbf{X}^\\mathrm{T})^{-1}$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabel = 'median house value in $'\n",
    "\n",
    "y = np.asarray(df_train.median_house_value)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to get the Observation matrix $\\mathbf{X}$.\n",
    "Take the vector `df_train.median_house_value` (1D-Array) and convert it to a matrix `X`, where you add a column with ones.\n",
    "\n",
    "Hints:\n",
    " - There are several solutions.\n",
    " - One solution is based on `np.stack` and `np.ones_like`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlabel = 'median income in 10 000 $'\n",
    "\n",
    "def get_observation_matrix(feature_vector):\n",
    "    X = np.stack([np.ones_like(feature_vector), feature_vector], axis=-1)  # REPLACE X = ???\n",
    "    return X\n",
    "\n",
    "X = get_observation_matrix(df_train.median_income)\n",
    "print(X[:5, :])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to estimate the parameter vector `theta`.\n",
    "\n",
    "Hint:\n",
    " \n",
    " - You may want to use `np.linalg.solve`, `np.linalg.inv`, `np.linalg.pinv` or `scipy.linalg.lstsq`.\n",
    " - Are some of them better than others? If yes, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit theta = np.linalg.pinv(X) @ y  # REPLACE\n",
    "%timeit theta, _, _, _ = scipy.linalg.lstsq(X, y)  # REPLACE\n",
    "%timeit theta = np.linalg.solve(X.T @ X, X.T @ y)  # REPLACE\n",
    "theta = np.linalg.solve(X.T @ X, X.T @ y)  # REPLACE theta = ???\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and test data and the regression model.\n",
    "\n",
    " - What do you expect?\n",
    " - Do you see, what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "def partial_plot(ax, dataframe, title, theta):\n",
    "    ax.scatter(dataframe.median_income, dataframe.median_house_value, alpha=0.01)\n",
    "    x_plot = np.linspace(0, 12)  # REPLACE ???\n",
    "    y_plot = get_observation_matrix(x_plot) @ theta   # REPLACE\n",
    "    ax.plot(x_plot, y_plot, color='red')  # REPLACE ax.plot(???, ???, color='red')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim((0, 12))\n",
    "    ax.set_ylim((0, 500000))\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "partial_plot(axes[0], df_train, 'Train set', theta)\n",
    "partial_plot(axes[1], df_test, 'Test set', theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the root mean squared error of your predictions both on the train and on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.asarray(df_test.median_house_value)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = get_observation_matrix(df_test.median_income)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(X, y, theta):# REPLACE\n",
    "    return np.sqrt((y - X @ theta) @ (y - X @ theta) / X.shape[0])  # REPLACE\n",
    "%timeit root_mean_squared_error(X, y, theta)  # REPLACE\n",
    "# REPLACE\n",
    "def root_mean_squared_error(X, y, theta):\n",
    "    difference = y - X @ theta  # REPLACE difference = ???\n",
    "    assert difference.shape == (X.shape[0],), 'Unexpected shape: {}'.format(difference.shape)\n",
    "    return np.sqrt((difference) @ difference / X.shape[0])  # REPLACE return ???\n",
    "\n",
    "%timeit root_mean_squared_error(X, y, theta)  # REPLACE root_mean_squared_error(X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your root_mean_squared_error implementation with a toy example\n",
    "rmse = root_mean_squared_error(\n",
    "    X=np.array([\n",
    "        [1, 2],\n",
    "        [1, 3],\n",
    "        [1, 4],\n",
    "    ]),\n",
    "    y=np.array([10, 12, 14]),\n",
    "    theta=np.array([6, 2])\n",
    ")\n",
    "assert rmse == 0, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train error: {:10.0f} US$'.format(\n",
    "    root_mean_squared_error(X, y, theta)\n",
    "))\n",
    "\n",
    "print('Test error:  {:10.0f} US$'.format(\n",
    "    root_mean_squared_error(X_test, y_test, theta)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a (stochastic) gradient descent algorithm to estimate the linear regression parameters.\n",
    "Ideally, write a function which allows to set different parameter, e.g. mini batch size.\n",
    "Add some kind of learning rate schedule (i.e. learning rate decreases by a factor of 10 after each epoch after the 5th).\n",
    "\n",
    "Hints:\n",
    " - Objective: $J = \\lVert\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}\\rVert^2$\n",
    " - Gradient: $\\displaystyle\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}}$\n",
    " - Gradient descent algorithm: $\\displaystyle\\boldsymbol{\\theta}^\\mathrm{new} = \\boldsymbol{\\theta}^\\mathrm{old} - \\mu \\left.\\frac{\\partial J}{\\partial \\boldsymbol{\\theta}}\\right|_{\\boldsymbol{\\theta} = \\boldsymbol{\\theta}^\\mathrm{old}}$ where $\\mu$ is the learning rate\n",
    " - Stochastic gradient descent algorithm: Use gradient descent algorithm with a subset of $\\mathbf{X}$ (i.e. mini batch)\n",
    "    - Important: Shuffle the training data. Often they are sorted and then the mini batch is far away to be representative for the complete data.\n",
    " - Epoch: One epoch means, that you processed all training data one time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions you may want to use:\n",
    " - `np.random.shuffle`\n",
    " - `np.array_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(epoch):\n",
    "    # This is a fixed learning rate schedule.\n",
    "    initial_learning_rate = 0.01  # REPLACE ???\n",
    "    if epoch <= 5:  # REPLACE return ???\n",
    "        return initial_learning_rate  # REPLACE\n",
    "    else:  # REPLACE\n",
    "        return initial_learning_rate * 10 ** -(epoch - 5)  # REPLACE\n",
    "\n",
    "def get_gradient_theta(X, y, theta):\n",
    "    batch_size = X.shape[0]  # REPLACE\n",
    "    return -(X.T @ y - X.T @ X @ theta) / batch_size  # REPLACE return ???\n",
    "\n",
    "def fit(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    epochs=10,\n",
    "    batch_size=10,\n",
    "    logging=True\n",
    "):\n",
    "    number_of_examples, number_of_parameters = X_train.shape\n",
    "    \n",
    "    # Set up batching parameters\n",
    "    all_example_indices = list(range(number_of_examples))\n",
    "    number_of_batches = -(-number_of_examples // batch_size)\n",
    "    \n",
    "    # Initialize variables\n",
    "    theta = np.zeros(number_of_parameters)\n",
    "    \n",
    "    # Prepare logging\n",
    "    if logging:\n",
    "        train_error_history = []\n",
    "        test_error_history = []\n",
    "        theta_history = []\n",
    "    else:\n",
    "        train_error_history = None\n",
    "        test_error_history = None\n",
    "        theta_history = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        learning_rate = get_learning_rate(epoch)\n",
    "        np.random.shuffle(all_example_indices)  # REPLACE ???  # shuffle all_example_indices\n",
    "        \n",
    "        for mini_batch in np.array_split(all_example_indices, number_of_batches):  # REPLACE ???  # for mini_batch in ???:\n",
    "            gradient = get_gradient_theta(  # REPLACE ???\n",
    "                X_train[mini_batch, :],  # REPLACE\n",
    "                y_train[mini_batch],  # REPLACE\n",
    "                theta  # REPLACE\n",
    "            )  # REPLACE\n",
    "            theta = theta - learning_rate * gradient  # REPLACE\n",
    "            \n",
    "            if logging:\n",
    "                theta_history.append(theta)\n",
    "                train_error_history.append(\n",
    "                    root_mean_squared_error(X_train, y_train, theta)\n",
    "                )\n",
    "                test_error_history.append(\n",
    "                    root_mean_squared_error(X_test, y_test, theta)\n",
    "                )\n",
    "    \n",
    "    if logging:\n",
    "        train_error_history = np.asarray(train_error_history)\n",
    "        test_error_history = np.asarray(test_error_history)\n",
    "        theta_history = np.asarray(theta_history)\n",
    "    \n",
    "    return theta, train_error_history, test_error_history, theta_history\n",
    "\n",
    "theta, train_error_history, test_error_history, theta_history = fit(\n",
    "    X, y,\n",
    "    X_test, y_test,\n",
    "    epochs=10,\n",
    "    batch_size=10,\n",
    "    logging=True\n",
    ")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(np.log(train_error_history), label='Train error')\n",
    "ax.plot(np.log(test_error_history), color='red', label='Test error')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Root Mean Squared Error')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(theta_history[:, 0], theta_history[:, 1])\n",
    "ax.scatter(theta[0], theta[1], color='red', zorder=3, s=100)\n",
    "ax.set_xlabel('w_0')\n",
    "ax.set_ylabel('w_1')\n",
    "\n",
    "plt.show()\n",
    "print('Best test loss: {:10.0f}'.format(np.min(test_error_history)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit  # REPLACE\n",
    "theta, train_error_history, test_error_history, theta_history = fit(  # REPLACE\n",
    "    X, y,  # REPLACE\n",
    "    X_test, y_test,  # REPLACE\n",
    "    epochs=10,  # REPLACE\n",
    "    batch_size=10,  # REPLACE\n",
    "    logging=False  # REPLACE\n",
    ")  # REPLACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit  # REPLACE\n",
    "theta, train_error_history, test_error_history, theta_history = fit(  # REPLACE\n",
    "    X, y,  # REPLACE\n",
    "    X_test, y_test,  # REPLACE\n",
    "    epochs=10,  # REPLACE\n",
    "    batch_size=10,  # REPLACE\n",
    "    logging=True  # REPLACE\n",
    ")  # REPLACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For experts...\n",
    "\n",
    "- Check the somewhat famous paper [1] from Yoshua Bengio. Especially Section 2 is worth a read. Also Section 3.1.1 provides useful information about learning rate schedules and other hyper-parameters.\n",
    "- Implement early stopping (stop when CV loss does not decrease for a few epochs).\n",
    "- Implement learning rate back-off based on CV loss.\n",
    "\n",
    "You can also add more features from the dataset to try to predict the prices better.\n",
    "How would you use the `ocean_proximity` feature.\n",
    "\n",
    "- [1] https://arxiv.org/pdf/1206.5533.pdf"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
